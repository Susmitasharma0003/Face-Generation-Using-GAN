{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3a1130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import zipfile\n",
    "import os\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "MAIN_PATH = r\"C:\\Users\\susmi\\Downloads\\img_align_celeba\\img_align_celeba\"  # Define the main path where images are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c103558",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = glob(MAIN_PATH+\"/*\") # Get all image file paths from the dataset folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfbffce",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(image_paths) # Count total number of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe48c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CelebA attributes dataset \n",
    "zip_path = r\"C:\\Users\\susmi\\Downloads\\list_bbox_celeba.csv.zip\"\n",
    "attributes_df = pd.read_csv(zip_path, compression='zip')\n",
    "attributes_df.head() # Show first few rows of attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a2a259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load landmarks dataset\n",
    "zip_path = r\"C:\\Users\\susmi\\Downloads\\list_landmarks_align_celeba.csv.zip\"\n",
    "landmarks_df = pd.read_csv(zip_path, compression='zip')\n",
    "landmarks_df.head() # Show first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0239340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load partition dataset \n",
    "zip_path = r\"C:\\Users\\susmi\\Downloads\\list_eval_partition.csv.zip\"\n",
    "partition = pd.read_csv(zip_path, compression='zip')\n",
    "partition.head() # Show first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565b2f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition['partition'].value_counts() # Count how many images belong to each partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4230546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train, validation, and test sets\n",
    "train_images = partition.query('partition == 0')\n",
    "valid_images = partition.query('partition == 1')\n",
    "test_images = partition.query('partition == 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12986170",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code For Discriminator\n",
    "discriminator = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(64, 64, 3)),    # Input image 64x64 RGB\n",
    "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"), # Conv layer\n",
    "        layers.LeakyReLU(negative_slope=0.2), # Activation function\n",
    "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(negative_slope=0.2),\n",
    "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(negative_slope=0.2),\n",
    "        layers.Flatten(),  # Flatten into 1D\n",
    "        layers.Dropout(0.2),   # Prevent overfitting\n",
    "        layers.Dense(1, activation=\"sigmoid\"),   # Output: real/fake\n",
    "    ],\n",
    "    name=\"discriminator\",\n",
    ")\n",
    "discriminator.summary()  # Show model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da01f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for generator\n",
    "\n",
    "latent_dim = 100  # Random noise dimension\n",
    "\n",
    "generator = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(latent_dim,)), # Input: noise vector\n",
    "        layers.Dense(8 * 8 * 512),  \n",
    "        layers.Reshape((8, 8, 512)),   # Reshape to 8x8x512\n",
    "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"generator\",\n",
    ")\n",
    "\n",
    "generator.summary()   # Show model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8cc9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images from directory and resize to 64x64\n",
    "dataset = keras.preprocessing.image_dataset_from_directory(\"archive\", label_mode=None, image_size=(64, 64), batch_size=32\n",
    ")\n",
    "dataset = dataset.map(lambda x: x / 255.0) # Normalize pixel values to [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ccd2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super(GAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(GAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
    "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
    "        \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.d_loss_metric, self.g_loss_metric]\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        # Sample random points in the latent space\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # Decode them to fake images\n",
    "        generated_images = self.generator(random_latent_vectors)\n",
    "\n",
    "        # Combine them with real images\n",
    "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
    "\n",
    "        # Assemble labels discriminating real from fake images\n",
    "        labels = tf.concat(\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "        # Add random noise to the labels - important trick!\n",
    "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
    "\n",
    "        # Train the discriminator\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # Assemble labels that say \"all real images\"\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights\n",
    "        # of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "        # Update metrics\n",
    "        self.d_loss_metric.update_state(d_loss)\n",
    "        self.g_loss_metric.update_state(g_loss)\n",
    "        return {\n",
    "            \"d_loss\": self.d_loss_metric.result(),\n",
    "            \"g_loss\": self.g_loss_metric.result(),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d5b2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, num_img=3, latent_dim=128):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    " # Save generated images after each epoch\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
    "        generated_images = self.model.generator(random_latent_vectors)\n",
    "        generated_images *= 255  # Rescale for saving\n",
    "        generated_images.numpy()\n",
    "        for i in range(self.num_img):\n",
    "            img = keras.preprocessing.image.array_to_img(generated_images[i])\n",
    "            img.save(\"generated_img_%03d_%d.png\" % (epoch, i))  # Save image\n",
    "            plt.savefig('image_at_epoch_{:04d}.png'.format(epoch)) # Save plot\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f3c287",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10  # Number of epochs\n",
    "\n",
    "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim) # Build GAN model\n",
    "\n",
    "# Compile with Adam optimizer and Binary Cross-Entropy loss\n",
    "gan.compile(\n",
    "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
    ")\n",
    "\n",
    "# Train GAN on dataset\n",
    "gan.fit(\n",
    "    dataset, epochs=epochs, callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7414ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a single image using the epoch number\n",
    "def display_image(epoch_no):\n",
    "  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979122f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all generated images\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "images = []\n",
    "for img_path in glob.glob('Generated Faces/*.png'):  # Load all generated images\n",
    "    images.append(mpimg.imread(img_path))\n",
    "    \n",
    "# Plot generated images in grid\n",
    "plt.figure(figsize=(20,10))\n",
    "columns = 20\n",
    "for i, image in enumerate(images):\n",
    "    plt.subplot(len(images) / columns + 1, columns, i + 1)\n",
    "    plt.axis('off')\n",
    "    #fig.tight_layout() \n",
    "    plt.imshow(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
